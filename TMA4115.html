<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Magne Tenstad" />
  <title>TMA4115 - Matematikk 3</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">TMA4115 - Matematikk 3</h1>
<p class="author">Magne Tenstad</p>
</header>
<h1 id="komplekse-tall">Komplekse tall</h1>
<p>Finner du ikke det du ser etter? Se <a href="https://www.math.ntnu.no/emner/TMA4110/2020h/notater/1-komplekse-tall.pdf">Wiki</a>.</p>
<h2 id="den-imaginære-enheten">Den imaginære enheten</h2>
<p>Vi definerer den imaginære enheten <span class="math inline">\(i\)</span>, ved <span class="math display">\[i^2 = -1\]</span> Merk at <span class="math inline">\(\sqrt{ab} = \sqrt{a} \, \sqrt{b}\)</span> bare gjelder for <span class="math inline">\(a, b &gt; 0\)</span>.<br />
<br />
Komplekse tall er tall på formen <span class="math display">\[z = a + bi\]</span> der <span class="math inline">\(a, b \in \mathbb{R}\)</span>, og <span class="math inline">\(i\)</span> er den imaginære enheten.<br />
<br />
Mengden av komplekse tall er <span class="math inline">\(\mathbb{C}\)</span>.</p>
<h2 id="operasjoner-på-komplekse-tall">Operasjoner på komplekse tall</h2>
<p>Regneregler for komplekse tall følger regnereglene for reelle tall.<br />
<br />
<strong>Definisjon.</strong> La <span class="math inline">\(z = a + bi\)</span> være et komplekst tall. Da er <span class="math inline">\(z\)</span> <em>konjugert</em> gitt ved <span class="math display">\[z = a - bi\]</span> Merk at <span class="math inline">\(z\overline{z} = a^2 + b^2\)</span> er et reelt tall.</p>
<h2 id="polare-koordinater">Polare koordinater</h2>
<p>Polarkoordinater er nyttige når en multipliserer og tar potenser av komplekse tall.<br />
<br />
La <span class="math inline">\(r\)</span> være avstanden fra det komplekse tallet <span class="math inline">\(z =
a + bi\)</span> til origo, og la <span class="math inline">\(\theta\)</span> være vinkelen <span class="math inline">\(z\)</span> gjør med den reelle aksen. Da har vi at <span class="math display">\[\begin{gathered}
    a = \text{Re}\, z = r \, \cos{\theta} \\
    b = \text{Im}\, z = r \, \sin{\theta}\end{gathered}\]</span> <span class="math inline">\(r\)</span> kalles <em>modulus</em> eller <em>absoluttverdi</em> til <span class="math inline">\(z\)</span> og <span class="math inline">\(\theta\)</span> kalles <em>vinkelen</em> eller <em>argumentet</em> til <span class="math inline">\(z\)</span>. <span class="math display">\[\begin{gathered}
    r = |z| = \sqrt{a^2 + b^2} = \sqrt{z\overline{z}} \\\\
    \theta =
    \begin{cases}
        \arctan \frac{b}{a} &amp; \text{for } a &gt; 0 \\
        \arctan \frac{b}{a} + \pi &amp; \text{for } a &lt; 0 \\
        \pi / 2 &amp; \text{for } a = 0, \, b &gt; 0 \\
        3\pi / 2 &amp; \text{for } a = 0, \, b &lt; 0 \\
    \end{cases}\end{gathered}\]</span></p>
<h3 id="trekantulikheten">Trekantulikheten</h3>
<p><strong>Teorem 1.5.</strong> La <span class="math inline">\(z\)</span> og <span class="math inline">\(w\)</span> være komplekse tall. Da gjelder at <span class="math display">\[|z + w| \leq |z| + |w|\]</span></p>
<h2 id="eulers-formel">Eulers formel</h2>
<p>Eulers formel er gitt ved <span class="math display">\[e^{ix} = \cos{x} + i \, \sin{x}\]</span> der <span class="math inline">\(x \in \mathbb{R}\)</span> og <span class="math inline">\(i\)</span> er den imaginære enheten.<br />
<br />
Det følger at <span class="math display">\[z = r(\cos{\theta} + i \, \sin{\theta}) = re^{i\theta}\]</span> <strong>Teorem 1.7.</strong> La <span class="math inline">\(z = re^{i\theta}\)</span> og <span class="math inline">\(w = se^{i\alpha}\)</span>. Da gjelder: <span class="math display">\[\begin{aligned}
    z \cdot w &amp;= rse^{i(\theta + \alpha)} \\
    \frac{z}{w} &amp;= \frac{r}{s} e^{i(\theta - \alpha)}\end{aligned}\]</span></p>
<h2 id="røtter-av-komplekse-tall">Røtter av komplekse tall</h2>
<p>Et polynom <span class="math inline">\(f(z) = z^n + a_{z-1}z^{n-1} + ... + a_1&lt;+a_0\)</span>, <span class="math inline">\(z \in \mathbb{C}\)</span> kan alltid faktoriseres <span class="math display">\[f(z) = \prod_{i=1}^n (z - z_i)\]</span> der <span class="math inline">\(z_i \in \mathbb{C}\)</span> er løsninger av likningen <span class="math inline">\(f(z) = 0\)</span>.</p>
<h1 id="lineære-likningssystemer-og-gausseliminasjon">Lineære likningssystemer og gausseliminasjon</h1>
<p>Finner du ikke det du ser etter? Se <a href="https://www.math.ntnu.no/emner/TMA4110/2020h/notater/2-lineare-likningssystemer.pdf">Wiki</a>.</p>
<h2 id="ekvivalente-systemer">Ekvivalente systemer</h2>
<p>Vi sier at to likningssystemer er ekvivalente dersom de har samme løsningsmengder.</p>
<h2 id="totalmatrisen-til-et-system">Totalmatrisen til et system</h2>
<p>Et lineært likningssystem med <span class="math inline">\(m\)</span> likninger og <span class="math inline">\(n\)</span> ukjente beskrives av en matrise. Denne kalles <em>totalmatrisen</em> eller <em>den utvidede matrisen</em> til likningssystemet. Vi lar gjerne en vertikal linje i matrisen skille venstre og høyre del av likningene.</p>
<h2 id="radoperasjoner">Radoperasjoner</h2>
<p>Følgende tre måter å endre en matrise på kalles <em>radoperasjoner</em>:</p>
<ol>
<li><p>Gange alle tallene i en rad med det samme tallet. Dette betyr å gange en likning med et tall. Vi kan ikke gange med 0.</p></li>
<li><p>Legge til et multiplum av en rad i en annen. Dette er å kombinere likninger til nye likninger.</p></li>
<li><p>Bytte rekkefølge på radene. Dette er det samme som å bytte rekkefølge på likningene.</p></li>
</ol>
<p>Vi sier at to matriser er radekvivalente hvis vi kan komme fra den ene til den andre ved å utføre en eller flere radoperasjoner. Vi bruker notasjonen <span class="math inline">\(M \sim N\)</span> for å si at to matriser <span class="math inline">\(M\)</span> og <span class="math inline">\(N\)</span> er radekvivalente.<br />
<br />
<strong>Teorem 2.7.</strong> Hvis to likningssystemer har radekvivalente totalmatriser, så er de to likningssystemene ekvivalente.</p>
<h2 id="trappeform-redusert-trappeform-og-pivotelementer">Trappeform, redusert trappeform og pivotelementer</h2>
<p><strong>Definisjon.</strong> Tallet lengst til venstre i en rad som ikke er <span class="math inline">\(0\)</span> kalles pivotelementet for den raden. (En rad med bare nuller har ikke noe pivotelement.)<br />
<br />
<strong>Definisjon.</strong> En matrise er på trappeform dersom det ikke er annet enn <span class="math inline">\(0\)</span>-ere under hvert pivotelement, og eventuelle nullrader er helt nederst.<br />
<br />
<strong>Definisjon.</strong> En matrise er på redusert trappeform hvis den er på trappeform, pivotelementene er <span class="math inline">\(1\)</span> og alle tall som står over pivotelementer er <span class="math inline">\(0\)</span>.<br />
<br />
<strong>Teorem 2.11.</strong> For enhver <span class="math inline">\(m \times n\)</span>-matrise <span class="math inline">\(M\)</span>, finnes en <span class="math inline">\(m \times n\)</span>-matrise <span class="math inline">\(N\)</span> på redusert trappeform, slik at <span class="math inline">\(M\)</span> og <span class="math inline">\(N\)</span> er radekvivalente.</p>
<h2 id="eksistens-entydighet-og-parametrisering-av-løsninger">Eksistens, entydighet og parametrisering av løsninger</h2>
<p>For ethvert lineært likningssystem må ett av de følgende punktene være sant:</p>
<ul>
<li><p>Systemet har ingen løsning: Dette skjer når vi i redusert trappeform har en rad av typen <span class="math inline">\(0 \ 0 \ ... \ 0 \ | \ b\)</span> der <span class="math inline">\(b \neq 0\)</span>.</p></li>
<li><p>Systemet har entydig løsning. Dette skjer når trappeform av totalmatrisen har pivotelement i alle kolonner unntatt den siste.</p></li>
<li><p>Systemet har uendelig mange løsninger. Dette skjer når vi får frie variabler. Vi får en fri variabel for hver kolonne (unntatt den siste) som ikke har pivotelement.</p></li>
</ul>
<h2 id="lineære-likninger-med-komplekse-tall">Lineære likninger med komplekse tall</h2>
<p>Et lineært likningssett med komplekse koeffisienter og løsning, kan løses med gausseliminasjon på samme måte som i det reelle tilfellet.</p>
<h1 id="vektorlikninger">Vektorlikninger</h1>
<p>Finner du ikke det du ser etter? Se <a href="https://www.math.ntnu.no/emner/TMA4110/2020h/notater/3-vektor-og-matriselikninger.pdf">Wiki</a>.</p>
<h2 id="vektorregning">Vektorregning</h2>
<p>En lineærkombinasjon av vektorene <span class="math inline">\(x\)</span> og <span class="math inline">\(y\)</span> er en vektor på formen <span class="math display">\[ax + by\]</span> der <span class="math inline">\(a\)</span> og <span class="math inline">\(b\)</span> er skalarer, kalt vekter.<br />
<br />
Det lineære spennet til <span class="math inline">\(x\)</span> og <span class="math inline">\(y\)</span> er mengden av <em>alle lineærkombinasjoner</em> av <span class="math inline">\(x\)</span> og <span class="math inline">\(y\)</span>.</p>
<h2 id="vektorlikninger-1">Vektorlikninger</h2>
<p>Likningssystemer kan ofte skrives om til vektorlikninger.</p>
<h2 id="geometrisk-tolkning-av-vektorlikninger-eksistens-og-entydighet-av-løsninger">Geometrisk tolkning av vektorlikninger: Eksistens og entydighet av løsninger</h2>
<p>Vi tar utgangspunkt i vektorlikningen <span class="math display">\[xv_1 + yv_2 + zv_3 = v_4\]</span> Vi skal nå gi noen geometriske illustrasjoner i <span class="math inline">\(\mathbb{R}^3\)</span> av hva som skjer i de ulike tilfellene:</p>
<ul>
<li><p>Systemet har ingen løsning: <span class="math inline">\(v_4 \notin \text{Sp}\{v_1, v_2, v_3\}\)</span></p></li>
<li><p>Systemet har entydig løsning: <span class="math inline">\(v_4 \in \text{Sp}\{v_1, v_2, v_3\}\)</span> og <span class="math inline">\(v_1, v_2, v_3\)</span> er lineært uavhengige.</p></li>
<li><p>Systemet har uendelig mange løsninger: <span class="math inline">\(v_4 \in \text{Sp}\{v_1, v_2, v_3\}\)</span> og <span class="math inline">\(v_1, v_2, v_3\)</span> er lineært avhengige.</p></li>
</ul>
<h1 id="matriser">Matriser</h1>
<p>Finner du ikke det du ser etter? Se <a href="https://www.math.ntnu.no/emner/TMA4110/2020h/notater/4-matriser.pdf">Wiki</a>.</p>
<h2 id="definisjoner-og-notasjon">Definisjoner og notasjon</h2>
<h2 id="produkt-av-matrise-og-vektor">Produkt av matrise og vektor</h2>
<p><strong>Teorem 4.6.</strong> Hvis <span class="math inline">\(A\)</span> er en <span class="math inline">\(m \times n\)</span>-matrise, <span class="math inline">\(v\)</span> og <span class="math inline">\(w\)</span> er vektorer i <span class="math inline">\(\mathbb{R}^n\)</span> og <span class="math inline">\(c\)</span> er en skalar, så har vi følgende likheter: <span class="math display">\[A(v + w) = Av + Aw\]</span> <span class="math display">\[A(cv) = c(Av)\]</span></p>
<h2 id="sum-og-skalering-av-matriser">Sum og skalering av matriser</h2>
<p><strong>Teorem 4.9.</strong> Hvis <span class="math inline">\(A\)</span> og <span class="math inline">\(B\)</span> er <span class="math inline">\(m \times n\)</span>-matriser, <span class="math inline">\(v\)</span> er en vektorer i <span class="math inline">\(\mathbb{R}^n\)</span> og <span class="math inline">\(c\)</span> er en skalar, så har vi følgende likheter: <span class="math display">\[(A + B)v = Av + Bv\]</span> <span class="math display">\[(cA)v = c(Av)\]</span></p>
<h2 id="matrisemultiplikasjon">Matrisemultiplikasjon</h2>
<p><strong>Definisjon.</strong> La A være en <span class="math inline">\(m \times n\)</span>-matrise med rader <span class="math inline">\(a_1, a_2, \dots, a_m\)</span>, og la B være en <span class="math inline">\(n \times p\)</span>-matrise med kolonner <span class="math inline">\(b_1, b_2, \dots, b_p\)</span>. Produktet av A og B er definert ved: <span class="math display">\[AB = \begin{bmatrix}
a_1b_1 &amp; a_1b_2 &amp; \cdots &amp; a_1b_p \\
a_1b_1 &amp; a_1b_2 &amp; \cdots &amp; a_1b_p \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
a_mb_1 &amp; a_mb_2 &amp; \cdots &amp; a_mb_p \\
\end{bmatrix}\]</span> <strong>Teorem 4.13.</strong> La <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>, og <span class="math inline">\(C\)</span> være matriser, <span class="math inline">\(v\)</span> en vektor, og <span class="math inline">\(c\)</span> et tall. I hver del av teoremet antar vi at størrelsene på matrisene og vektoren er slik at alle operasjonene som brukes er definert.<br />
<br />
(a) Matrisemultiplikasjon er en assosiativ operasjon, det vil si: <span class="math display">\[A(BC) = (AB)C\]</span> Et spesialtilfelle av dette er følgende: <span class="math display">\[(AB)v = A(Bv)\]</span> (b) Å skalere et matriseprodukt er det samme som å skalene én av faktorene og deretter multiplisere: <span class="math display">\[(cA)B = c(AB) = A(cB)\]</span> (c) Matrisemultiplikasjon distribuerer over addisjon av matriser, det vil si: <span class="math display">\[A(B+C) = AB + AC\]</span> <span class="math display">\[(A + B)C = AC +\ BC\]</span></p>
<h2 id="transponering">Transponering</h2>
<p>Operasjonen <em>transponering</em> går ut på å bytte om rader og kolonner.<br />
<br />
<strong>Teorem 4.15.</strong> For enhver matrise <span class="math inline">\(A\)</span> har vi: <span class="math display">\[(A^\top)^\top = A\]</span> Hvis <span class="math inline">\(A\)</span> og <span class="math inline">\(B\)</span> er matriser slik at produktet <span class="math inline">\(AB\)</span> er definert, så er: <span class="math display">\[(AB)^\top = B^\top A^\top\]</span></p>
<h2 id="identitetsmatriser">Identitetsmatriser</h2>
<p>La <span class="math inline">\(I = I_n\)</span> være den kvadratiske <span class="math inline">\(n \times n\)</span>-matrisen <span class="math display">\[I_n = \begin{bmatrix}
1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; 1 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; 1
\end{bmatrix}\]</span> Da er <span class="math inline">\(IA = A\)</span> for enhver <span class="math inline">\(n \times p\)</span>-matrise <span class="math inline">\(A\)</span> og <span class="math inline">\(BI = B\)</span> for enhver <span class="math inline">\(m \times n\)</span>-matrise <span class="math inline">\(B\)</span>. Spesielt er <span class="math inline">\(Ix = x\)</span> for enhver vektor <span class="math inline">\(x\)</span>. Derfor kalles <span class="math inline">\(I = I_n\)</span> en identitetsmatrise, eller en <span class="math inline">\(n \times n\)</span>-identitetsmatrise.</p>
<h2 id="potenser-av-matriser">Potenser av matriser</h2>
<p>Hvis <span class="math inline">\(A\)</span> er en kvadratisk matrise, så kan vi gange <span class="math inline">\(A\)</span> med seg selv. Generelt definerer vi at <span class="math inline">\(A\)</span> opphøyd i <span class="math inline">\(n\)</span>-te er produktet av <span class="math inline">\(A\)</span> med seg selv <span class="math inline">\(n\)</span> ganger: <span class="math display">\[A^n = A \cdot A \cdot \cdots \cdot A\]</span> For tall har vi definert at <span class="math inline">\(a^0 = 1\)</span>. For kvadratiske matriser har vi derimot <span class="math display">\[A^0 = I_n\]</span></p>
<h2 id="inverser">Inverser</h2>
<p><strong>Definisjon.</strong> La <span class="math inline">\(A\)</span> være en <span class="math inline">\(n \times\)</span>-matrise. En <em>invers</em> til <span class="math inline">\(A\)</span> er en <span class="math inline">\(n \times n\)</span>-matrise <span class="math inline">\(B\)</span> som er slik at <span class="math display">\[A \cdot B = I_n = B \cdot A\]</span> En matrise er <em>inverterbar</em> hvis den har en invers.<br />
<br />
<strong>Teorem 4.18.</strong> Hvis en matrise er inverterbar, så har den nøyaktig én invers.<br />
<br />
<strong>Teorem 4.20.</strong> La <span class="math inline">\(A\)</span> være en <span class="math inline">\(n \times n\)</span>-matrise, og <span class="math inline">\(b\)</span> en vektor. Hvis <span class="math inline">\(A\)</span> er inverterbar, så har likningen <span class="math inline">\(Ax = b\)</span> entydig løsning, og løsningen er <span class="math inline">\(x = A^{-1}b\)</span>.</p>
<h2 id="beregning-av-inverser">Beregning av inverser</h2>
<p><strong>Teorem 4.25.</strong> La <span class="math inline">\(A\)</span> være en <span class="math inline">\(n \times n\)</span>-matrise.<br />
<br />
(a) <span class="math inline">\(A\)</span> er inverterbar hvis og bare hvis <span class="math inline">\(A \sim I_n\)</span>.<br />
<br />
(b) Hvis <span class="math inline">\(A\)</span> er inverterbar, så kan vi finne inversen ved å gausseliminere matrisen <span class="math display">\[\begin{bmatrix} A \ | \ I_n \end{bmatrix}\]</span> til redusert trappeform og lese av høyre halvdel av den resulterende matrisen. Med andre ord: Resultatet av gausseliminasjonen blir følgende matrise: <span class="math display">\[\begin{bmatrix} I_n \ | \ A^{-1} \end{bmatrix}\]</span></p>
<h2 id="formel-for-invertering-av-2-times-2-matrise">Formel for invertering av <span class="math inline">\(2 \times 2\)</span>-matrise</h2>
<p>La <span class="math display">\[A = \begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix}\]</span> Da er <span class="math display">\[A^{-1} = \frac{1}{ad-bc} \, \begin{bmatrix} d &amp; -b \\ -c &amp; a \end{bmatrix}\]</span></p>
<h1 id="lineær-uavhengighet">Lineær uavhengighet</h1>
<p>Finner du ikke det du ser etter? Se <a href="https://www.math.ntnu.no/emner/TMA4110/2020h/notater/5-linear-uavhengighet.pdf">Wiki</a>.</p>
<h2 id="lineært-spenn-overflødige-vektorer">Lineært spenn: overflødige vektorer</h2>
<p>Dersom en vektor <span class="math inline">\(u\)</span> ikke utvider spennet til en mengde vektorer <span class="math inline">\(v_1, \cdots, v_n\)</span> så er vektorene <span class="math inline">\(u, v_1, \cdots, v_n\)</span> lineært avhengige.</p>
<h2 id="definisjon-av-lineær-uavhengighet">Definisjon av lineær uavhengighet</h2>
<p><strong>Definisjon.</strong> Vektorene <span class="math inline">\(v_1, v_2, \cdots, v_n\)</span> er lineært uavhengige dersom likningen <span class="math display">\[x_1 v_1 + x_2 v_2 + \cdots + x_n v_n = 0\]</span> ikke har andre løsninger enn den trivielle løsningen <span class="math inline">\(x_1 = x_2 = \cdots = x_n = 0\)</span>. I motsatt tilfelle kalles de lineært avhengige.</p>
<h2 id="lineær-uavhengighet-for-to-vektorer">Lineær uavhengighet for to vektorer</h2>
<p><strong>Teorem 5.6.</strong> To vektorer <span class="math inline">\(u\)</span> og <span class="math inline">\(v\)</span>, begge ulik <span class="math inline">\(0\)</span>, er lineært uavhengige hvis og bare hvis <span class="math inline">\(u \neq cv\)</span>, for en skalar <span class="math inline">\(c \neq 0\)</span>.</p>
<h2 id="hvordan-sjekke-lineær-uavhengighet">Hvordan sjekke lineær uavhengighet?</h2>
<p><strong>Teorem 5.8.</strong> La <span class="math inline">\(A\)</span> være en matrise. Følgende påstander er ekvivalente:</p>
<ol>
<li><p>Kolonnene i <span class="math inline">\(A\)</span> er lineært uavhengige.</p></li>
<li><p>Likningen <span class="math inline">\(Ax = 0\)</span> har bare den trivielle løsningen <span class="math inline">\(x = 0\)</span>.</p></li>
<li><p>Vi får ingen frie variabler når vi løser <span class="math inline">\(Ax = 0\)</span>.</p></li>
<li><p>Når vi gausseliminerer <span class="math inline">\(A\)</span>, får vi et pivotelement i hver kolonne.</p></li>
</ol>
<p>Teorem 5.8 gir oss en grei metode for å sjekke lineær uavhengighet. Hvis vi har vektorer <span class="math display">\[v_1, v_2, \dots, v_n\]</span> kan vi finne ut om de er lineært uavhengige på denne måten:</p>
<ol>
<li><p>Lag en matrise <span class="math inline">\(A = [ \ v_1 \ v_2 \ \dots \ v_n \ ]\)</span> med disse vektorene som kolonner.</p></li>
<li><p>Gausseliminer <span class="math inline">\(A\)</span> til trappeform.</p></li>
<li><p>Hvis hver kolonne inneholder et pivotelement, er vektorene lineært uavhengige. Ellers er de lineært avhengige.</p></li>
</ol>
<p><strong>Teorem 5.12.</strong> Gitt <span class="math inline">\(n\)</span> vektorer <span class="math inline">\(v_1, v_2, \dots, v_n\)</span> i <span class="math inline">\(\mathbb{R}^m\)</span> eller <span class="math inline">\(\mathbb{C}^m\)</span>. Hvis</p>
<ol>
<li><p>en av vektorene er en lineærkombinasjon av de andre, eller</p></li>
<li><p><span class="math inline">\(n &gt; m\)</span>,</p></li>
</ol>
<p>så er vektorene lineært avhengige.<br />
<br />
<strong>Teorem 5.13.</strong> Vektorene <span class="math inline">\(v_1, v_2, \dots, v_n\)</span> er lineært uavhengige hvis og bare hvis ingen av dem kan skrives som en lineærkombinasjon av de andre.</p>
<h2 id="like-mange-vektorer-som-dimensjonen">Like mange vektorer som dimensjonen</h2>
<p><strong>Teorem 5.14.</strong> Hvis vi har <span class="math inline">\(n\)</span> vektorer <span class="math inline">\(v_1, v_2, \dots, v_n\)</span> i <span class="math inline">\(\mathbb{R}^n\)</span>, så er de lineært uavhengige hvis og bare hvis de utspenner hele <span class="math inline">\(\mathbb{R}^n\)</span>, altså hvis og bare hvis <span class="math display">\[\text{Sp}\{v_1, v_2, \dots, v_n \} = \mathbb{R}^n\]</span></p>
<h1 id="determinanter">Determinanter</h1>
<p>Finner du ikke det du ser etter? Se <a href="https://www.math.ntnu.no/emner/TMA4110/2020h/notater/6-determinanter.pdf">Wiki</a>.</p>
<h2 id="determinanter-for-2-x-2-matriser">Determinanter for 2 x 2-matriser</h2>
<p>For en <span class="math inline">\(2 \times 2\)</span>-matrise <span class="math display">\[A = \begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix}\]</span> er determinanten definert ved: <span class="math display">\[\det{A} = \begin{vmatrix} a &amp; b \\ c &amp; d \end{vmatrix} = ad - bc\]</span> Arealet av parallellogrammet utspent av kolonnene i <span class="math inline">\(A\)</span> er lik <span class="math display">\[| \det{A} |\]</span></p>
<h2 id="determinanter-for-3-x-3-matriser">Determinanter for 3 x 3-matriser</h2>
<p>For en <span class="math inline">\(3 \times 3\)</span>-matrise <span class="math display">\[A =
\begin{bmatrix}
a_{11} &amp; a_{12} &amp; a_{13} \\
a_{21} &amp; a_{22} &amp; a_{23} \\
a_{31} &amp; a_{32} &amp; a_{33}
\end{bmatrix}\]</span> er determinanten definert ved: <span class="math display">\[\begin{aligned}
    \det{A} &amp;=
    \begin{vmatrix}
    a_{11} &amp; a_{12} &amp; a_{13} \\
    a_{21} &amp; a_{22} &amp; a_{23} \\
    a_{31} &amp; a_{32} &amp; a_{33}
    \end{vmatrix} \\
    &amp;= a_{11}\begin{vmatrix} a_{22} &amp; a_{23} \\ a_{32} &amp; a_{33} \end{vmatrix}
    - a_{12}\begin{vmatrix} a_{21} &amp; a_{23} \\ a_{31} &amp; a_{33} \end{vmatrix}
    + a_{13}\begin{vmatrix} a_{21} &amp; a_{22} \\ a_{31} &amp; a_{32} \end{vmatrix}\\
    &amp;= a_1 \cdot a_2 \times a_3 = |a_1|\,|a_2|\,|a_3|\,\sin{\alpha}\cos{\theta}\end{aligned}\]</span></p>
<h2 id="determinanter-og-radoperasjoner">Determinanter og radoperasjoner</h2>
<p><strong>Teorem 6.5.</strong> La <span class="math inline">\(A\)</span> være en <span class="math inline">\(n \times n\)</span>-matrise, og la <span class="math inline">\(B\)</span> være en matrise vi får ved å utføre en radoperasjon på <span class="math inline">\(A\)</span>. Da har vi følgende sammenheng mellom determinantene til <span class="math inline">\(A\)</span> og <span class="math inline">\(B\)</span>, basert på hvilken type radoperasjon vi utførte:</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Radoperasjon</th>
<th style="text-align: center;">Resultat</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Gange en rad med et tall <span class="math inline">\(k\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\det{B} = k \cdot \det{A}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">Legge til et multiplum av én rad i en annen</td>
<td style="text-align: center;"><span class="math inline">\(\det{B} = \det{B}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">Bytte om to rader</td>
<td style="text-align: center;"><span class="math inline">\(\det{B} = -\det{A}\)</span></td>
</tr>
</tbody>
</table>
<h2 id="triangulære-matriser">Triangulære matriser</h2>
<p><strong>Teorem 6.8.</strong> La <span class="math inline">\(A\)</span> være en (øvre eller nedre) triangulær <span class="math inline">\(n \times n\)</span>-matrise. Da er determinanten til <span class="math inline">\(A\)</span> lik produktet av tallene på diagonalen i <span class="math inline">\(A\)</span>: <span class="math display">\[\det{A} = a_{11} \cdot a_{22} \cdot \cdots \cdot a_{nn}\]</span> <strong>Teorem fra LF</strong> La <span class="math inline">\(A\)</span> være en øvre triangulær <span class="math inline">\(n \times n\)</span>-matrise. Da er egenverdiene til <span class="math inline">\(A\)</span> elementene på diagonalen.</p>
<h2 id="flere-regneregler-for-determinanter">Flere regneregler for determinanter</h2>
<p><strong>Teorem 6.10.</strong> Determinanten til et produkt av to matriser er produktet av determinantene. Altså: Hvis <span class="math inline">\(A\)</span> og <span class="math inline">\(B\)</span> er to <span class="math inline">\(n \times n\)</span>-matriser, så er <span class="math display">\[\det{AB} = (\det{A})(\det{B})\]</span> <strong>Teorem 6.11.</strong> Determinanten endrer seg ikke når vi transponerer matrisen. Altså: Hvis <span class="math inline">\(A\)</span> er en <span class="math inline">\(n \times n\)</span>-matrise, så er <span class="math display">\[\det{A} = \det{A^{\top}}\]</span></p>
<h2 id="karakterisering-av-inverterbarhet">Karakterisering av inverterbarhet</h2>
<p><strong>Teorem 6.12.</strong> La <span class="math inline">\(A\)</span> være en <span class="math inline">\(n \times n\)</span>-matrise. Følgende påstander er ekvivalente:</p>
<ol>
<li><p><span class="math inline">\(A\)</span> er inverterbar.</p></li>
<li><p><span class="math inline">\(\det{A} \neq 0\)</span>.</p></li>
<li><p>Kolonnene i <span class="math inline">\(A\)</span> er lineært uavhengige.</p></li>
<li><p>Kolonnene i <span class="math inline">\(A\)</span> utspenner <span class="math inline">\(\mathbb{C}^n\)</span>.</p></li>
</ol>
<h1 id="vektorrom">Vektorrom</h1>
<p>Finner du ikke det du ser etter? Se <a href="https://www.math.ntnu.no/emner/TMA4110/2020h/notater/7-vektorrom.pdf">Wiki</a>.</p>
<h2 id="definisjonen-av-vektorrom">Definisjonen av vektorrom</h2>
<p><strong>Definisjon.</strong> La <span class="math inline">\(V\)</span> være en mengde, og anta at vi har definert to operasjoner: <span class="math display">\[\begin{gathered}
    \text{addisjon av vektorer:} \ u + v \\
    \text{skalarmultiplikasjon:} \ c \cdot u\end{gathered}\]</span> Addisjon skal være definert for alle elementer <span class="math inline">\(u\)</span> og <span class="math inline">\(v\)</span> i <span class="math inline">\(V\)</span>, og skalarmultiplikasjonen for alle skalarer <span class="math inline">\(c\)</span> og alle <span class="math inline">\(u\)</span> i <span class="math inline">\(V\)</span>. Resultatet av operasjonene skal alltid være et element i <span class="math inline">\(V\)</span>. Dersom mengden <span class="math inline">\(V\)</span> og de to operasjonene oppfyller vektorromsaksiomene (V1)-(V8), så sier vi at <span class="math inline">\(V\)</span> er et <em>vektorrom</em>, og vi kaller elementene i <span class="math inline">\(V\)</span> for <em>vektorer</em>.</p>
<h2 id="generelle-egenskaper-og-lineær-uavhengighet">Generelle egenskaper og lineær uavhengighet</h2>
<p><strong>Definisjon.</strong> La <span class="math inline">\(v_1, v_2, \dots, v_n\)</span> være vektorer i vektorommet <span class="math inline">\(V\)</span>. Disse vektorene er <em>lineært</em> uavhengige dersom likningen <span class="math display">\[c_1 \cdot v_1 + c_2 \cdot v_2 + \cdots + c_n \cdot v_n = 0\]</span> ikke har andre lsninger enn den trivielle løsningen <span class="math inline">\(c_1 = c_2 = \cdots = c_n = 0.\)</span> I motsatt tilfelle kalles de <em>lineært avhengige</em>.<br />
<br />
<strong>Teorem 7.3.</strong> Gitt <span class="math inline">\(n\)</span> vektorer <span class="math inline">\(v_1, v_2, \dots, v_n\)</span> i et vektorrom <span class="math inline">\(V\)</span>. Hvis</p>
<ol>
<li><p>en av vektorene er en lineærkombinasjon av de andre, eller</p></li>
<li><p>en av vektorene er <span class="math inline">\(0\)</span>,</p></li>
</ol>
<p>så er vektorene lineært avhengige.</p>
<h2 id="flere-eksempler-på-vektorrom">Flere eksempler på vektorrom</h2>
<ul>
<li><p>Polynomer av begrenset grad.</p></li>
<li><p>Alle polynomer.</p></li>
<li><p>Kontinuerlige funksjoner.</p></li>
<li><p>Deriverbare og glatte funksjoner.</p></li>
<li><p>Matriser.</p></li>
</ul>
<h2 id="underrom">Underrom</h2>
<p><strong>Definisjon.</strong> Et <em>underrom</em> av et vektorrom <span class="math inline">\(V\)</span> er en delmengde <span class="math inline">\(U \subseteq V\)</span> som i seg selv utgjør et vektorrom, med addisjon og skalarmultipliskasjon definert på samme måte som i <span class="math inline">\(V\)</span>.<br />
<br />
<strong>Teorem 7.9.</strong> La <span class="math inline">\(V\)</span> være et vektorrom. En delmengde <span class="math inline">\(U \subseteq V\)</span> er et underrom av <span class="math inline">\(V\)</span> hvis og bare hvis følgende tre betingelser er oppfylt.</p>
<ol>
<li><p>Nullvektoren <span class="math inline">\(0\)</span> i <span class="math inline">\(V\)</span> ligger i <span class="math inline">\(U\)</span>.</p></li>
<li><p>For alle vektorer <span class="math inline">\(u\)</span> og <span class="math inline">\(v\)</span> i <span class="math inline">\(U\)</span> er også summen <span class="math inline">\(u + v\)</span> i <span class="math inline">\(U\)</span>.</p></li>
<li><p>For alle vektorer <span class="math inline">\(u\)</span> i <span class="math inline">\(U\)</span> og alle skalarer <span class="math inline">\(c\)</span> er også skalarproduktet <span class="math inline">\(cu\)</span> u <span class="math inline">\(U\)</span>.</p></li>
</ol>
<p><strong>Teorem 7.11</strong> En mengde <span class="math inline">\(\text{Sp}\{v_1, v_2, \dots, v_n\}\)</span> utspent av vektorer i et vektorrom <span class="math inline">\(V\)</span> er alltid et underrom av <span class="math inline">\(V\)</span>.</p>
<h2 id="endeligdimensjonale-vektorrom">Endeligdimensjonale vektorrom</h2>
<p><strong>Definisjon.</strong> Et vektorrom <span class="math inline">\(V\)</span> er <em>endeligdimensjonalt</em> hvis det finnes en endelig mengde av vektorer i <span class="math inline">\(V\)</span> som utspenner <span class="math inline">\(V\)</span>. Ellers er <span class="math inline">\(V\)</span> <em>uendeligdimensjonalt</em>.</p>
<h2 id="basis">Basis</h2>
<p><strong>Definisjon.</strong> En <em>basis</em> for et vektorrom <span class="math inline">\(V\)</span> er en liste <span class="math display">\[B = (b_1, b_2, \dots, b_n)\]</span> av vektorer i <span class="math inline">\(V\)</span> som både utspenner <span class="math inline">\(V\)</span> og er lineært uavhengige.<br />
<br />
<strong>Teorem 7.14.</strong> La <span class="math inline">\(V\)</span> være et vektorrom med basis <span class="math display">\[B = (b_1, b_2, \dots, b_n)\]</span> Da kan hver vektor <span class="math inline">\(v\)</span> i <span class="math inline">\(V\)</span> skrives som en lineærkombinasjon <span class="math display">\[v = c_1b_1 + c_2b_2 + \cdots + c_nb_n\]</span> av basisvektorene i <span class="math inline">\(B\)</span>, på en entydig måte.<br />
<br />
<strong>Definisjon.</strong> Tallene <span class="math inline">\(c_1, c_2, \dots, c_n\)</span> i teorem 7.14 kalles <em>koordinatene</em> til vektoren <span class="math inline">\(v\)</span> <em>med hensyn på</em> basisen <span class="math inline">\(B\)</span>. Vi definerer notasjonen <span class="math inline">\([\,v\,]_B\)</span> for vektoren i <span class="math inline">\(\mathbb{C}^n\)</span> som består av koordinatene til v: <span class="math display">\[[\,v\,]_B = \begin{bmatrix}
    c_1 \\ c_2 \\ \vdots \\ c_n
\end{bmatrix}\]</span> <strong>Teorem 7.16.</strong> La <span class="math inline">\(V\)</span> være et vektorrom med basis <span class="math inline">\(B\)</span>. Koordinatene til en lineærkombinasjon av vektorer er tilsvarende lineærkombinasjonen av koordinatene til hver vektor: <span class="math display">\[[\, c_1v_1 + c_2v_2 + \cdots + c_tv_t\,]_B = c_1 \cdot [\,v\,]_B + c_2 \cdot [\,v_2\,]_B + \cdots + c_t \cdot [\,v_t\,]_B\]</span> <strong>Teorem 7.18.</strong> La <span class="math inline">\(V\)</span> være et endeligdimensjonalt vektorrom. Da kan enhver endelig mengde som utspenner <span class="math inline">\(V\)</span> reduseres til en basis for <span class="math inline">\(V\)</span>. Mer presist: Hvis <span class="math inline">\(G\)</span> er en endelig mengde av vektorer slik at <span class="math inline">\(\text{Sp}\,G = V\)</span>, så finnes en delmengde <span class="math inline">\(B \subseteq G\)</span> slik at vektorene i <span class="math inline">\(B\)</span> utgjør en basis for <span class="math inline">\(V\)</span>.<br />
<br />
<strong>Teorem 7.19.</strong> Ethvert endeligdimensjonalt vektorrom har en basis.<br />
<br />
<strong>Teorem 7.20.</strong> La <span class="math inline">\(V\)</span> være et endeligdimensjonalt vektorrom. Enhver endelig mengde av vektorer i <span class="math inline">\(V\)</span> som er lineært uavhengig kan utvides til en basis. Mer presist: Hvis <span class="math inline">\(L\)</span> er en endelig mengde av vektorer som er lineært uavhengige, så finees en basis for <span class="math inline">\(V\)</span> som inneholder alle vektorene i <span class="math inline">\(L\)</span>.</p>
<h2 id="dimensjon">Dimensjon</h2>
<p><strong>Teorem 7.21.</strong> La <span class="math inline">\(V\)</span> være et vektorrom med en basis <span class="math inline">\(B\)</span> som består av <span class="math inline">\(n\)</span> vektorer. La <span class="math inline">\(v_1, v_2, \dots, v_m\)</span> være <span class="math inline">\(m\)</span> vektorer i <span class="math inline">\(V\)</span>, der <span class="math inline">\(m &gt; n\)</span>. Da er disse vektorene lineært avhengige.<br />
<br />
<strong>Teorem 7.22.</strong> La <span class="math inline">\(V\)</span> være et endeligdimensjonalt vektorrom. Da har enhver basis for <span class="math inline">\(V\)</span> samme størrelse.<br />
<br />
<strong>Definisjon.</strong> La <span class="math inline">\(V\)</span> være et endeligdimensjonalt vektorrom. Vi definerer <em>dimensjonen</em> til <span class="math inline">\(V\)</span> som antall vektorer i en basis for <span class="math inline">\(V\)</span>. Vi bruker notasjonen <span class="math inline">\(\dim{V}\)</span> for dimensjonen til <span class="math inline">\(V\)</span>. Hvis <span class="math inline">\(B\)</span> er en basis for <span class="math inline">\(V\)</span>, har vi altså <span class="math display">\[\dim{V} = |B|\]</span><br />
<br />
<strong>Teorem 7.23.</strong> La <span class="math inline">\(V\)</span> være et vektorrom med et underrom <span class="math inline">\(U\)</span>. Hvis <span class="math inline">\(V\)</span> er endeligdimensjonalt, så er <span class="math inline">\(U\)</span> også endeligdimensjonalt, og <span class="math display">\[\dim{U} \leq \dim{V}\]</span></p>
<h2 id="vektorrom-tilknyttet-en-matrise">Vektorrom tilknyttet en matrise</h2>
<p><strong>Nullrommet.</strong> Vi definerer <em>nullrommet</em> til en reell <span class="math inline">\(m \times n\)</span>-matrise <span class="math inline">\(A\)</span> som løsningsmengden til likningen <span class="math inline">\(Ax = 0\)</span>, altså delmengden av <span class="math inline">\(\mathbb{R}^n\)</span>: <span class="math display">\[\text{Null}\,A = \{ x \in \mathbb{R}^n \,|\, Ax = 0 \}\]</span> <strong>Kolonnerommet.</strong> Vi definerer <em>kolonnerommet</em> til en reell <span class="math inline">\(m \times n\)</span>-matrise <span class="math display">\[A = [\, a_1 \ a_2 \ \cdots \ a_n \,]\]</span> som underrommet av <span class="math inline">\(\mathbb{R}^m\)</span> utspent av kolonnene i <span class="math inline">\(A\)</span>: <span class="math display">\[\text{Col}\,A = \text{Sp} \{\, a_1, a_2, \dots, a_n \,\}\]</span> Vi kan også beskrive kolonnerommet ved <span class="math display">\[\text{Col}\,A = \{\, Av \,|\, v \in \mathbb{R}^n \,\}\]</span> Vi finner en basis for kolonnerommet ved:</p>
<ol>
<li><p>Bruk gausseliminasjon for å finne trappeformmatrisen <span class="math inline">\(A&#39;\)</span> som er radekvivalant til <span class="math inline">\(A\)</span>.</p></li>
<li><p>Se på <span class="math inline">\(A&#39;\)</span> for å finne ut hvilke av kolonnene i <span class="math inline">\(A\)</span> som er pivotkolonner.</p></li>
<li><p>Pivotkolonnene danner en basis for <span class="math inline">\(\text{Col}\,A\)</span>.</p></li>
</ol>
<p><strong>Radrommet.</strong> Vi definerer <em>radrommet</em> til en reell <span class="math inline">\(m \times n\)</span>-matrise <span class="math display">\[A = \begin{bmatrix} r_1^\top \\ r_2^\top \\ \vdots \\ r_m^\top \end{bmatrix}\]</span> som underrommet av <span class="math inline">\(\mathbb{R}^n\)</span> utspent av radene i <span class="math inline">\(A\)</span>: <span class="math display">\[\text{Row}\,A = \text{Sp} \{\, r_1, r_2, \dots, r_m \,\}\]</span> Dette er det samme som kolonnerommet til den transponerte matrisen: <span class="math display">\[\text{Row}\,A = \text{Col}\,A^\top\]</span> <strong>Teorem 7.25</strong> La <span class="math inline">\(A\)</span> være en <span class="math inline">\(m \times n\)</span>-matrise, og la <span class="math inline">\(E\)</span> være trappeformmatrisen vi får når vi gausseliminerer <span class="math inline">\(A\)</span>. Da har vi:</p>
<ol>
<li><p>Dimensjonen til nullrommet til <span class="math inline">\(A\)</span> er lik antall frie variabler vi får når vi løser likningssystemet <span class="math inline">\(Ax = 0\)</span>, altså antall kolonner uten pivotelementer i <span class="math inline">\(E\)</span>.</p></li>
<li><p>Dimensjonen til kolonnerommet til <span class="math inline">\(A\)</span> er lik antall kolonner med pivotelementer i <span class="math inline">\(E\)</span>.</p></li>
<li><p>Dimensjonen til radrommet til <span class="math inline">\(A\)</span> er lik antall rader som ikke er null i <span class="math inline">\(E\)</span>.</p></li>
</ol>
<p><strong>Teorem 7.26</strong> La <span class="math inline">\(A\)</span> være en <span class="math inline">\(m \times n\)</span>-matrise. Da har kolonnerommet og radrommet til <span class="math inline">\(A\)</span> samme dimensjon. Dette kalles <em>rangen</em> til <span class="math inline">\(A\)</span>. <span class="math display">\[\text{dim Col}\,A = \text{dim Row}\,A = \text{rank}\,A\]</span> <strong>Teorem 7.27</strong> La <span class="math inline">\(A\)</span> være en <span class="math inline">\(m \times n\)</span>-matrise. Da er <span class="math display">\[\text{dim Null}\,A + \text{rank}\,A = n\]</span></p>
<h1 id="lineærtransformasjoner">Lineærtransformasjoner</h1>
<p>Finner du ikke det du ser etter? Se <a href="https://www.math.ntnu.no/emner/TMA4110/2020h/notater/8-lineartransformasjoner.pdf">Wiki</a>.</p>
<h2 id="funksjoner">Funksjoner</h2>
<p><strong>Definisjon.</strong> En <em>funksjon</em> består av:</p>
<ol>
<li><p>En mengde som kalles funksjonens <em>domene</em>.</p></li>
<li><p>En mengde som kalles funksjonens <em>kodomene</em>.</p></li>
<li><p>En regel som til hvert element i domenet tilordner et element i kodomenet.</p></li>
</ol>
<p><strong>Definisjon.</strong> La <span class="math inline">\(f: A \rightarrow B\)</span> være en funksjon. Vi sier at <span class="math inline">\(f\)</span> er <em>injektiv</em> (eller <em>en-til-en</em>) hvis det for hver <span class="math inline">\(b\)</span> i <span class="math inline">\(B\)</span> er maksimalt én <span class="math inline">\(a\)</span> i <span class="math inline">\(A\)</span> slik at <span class="math inline">\(f(a) = b\)</span>. Vi sier at <span class="math inline">\(f\)</span> er <em>surjektiv</em> (eller <em>på</em>) hvis det for hver <span class="math inline">\(b\)</span> i <span class="math inline">\(B\)</span> finnes en <span class="math inline">\(a\)</span> i <span class="math inline">\(A\)</span> slik at <span class="math inline">\(f(a) = b\)</span>. <em>Bildet</em> til <span class="math inline">\(f\)</span> er mengden av alle elementer i kodomenet som blir truffet av <span class="math inline">\(f\)</span>, altså delmengden <span class="math inline">\(\text{im} f = \{\, f(a) \ | \ a \in A \}\)</span> av <span class="math inline">\(B\)</span>.<br />
<br />
Det følger umiddelbart fra definisjonen at en funksjon <span class="math inline">\(f: A \rightarrow B\)</span> er surjektiv hvis og bare hvis bildet til funksjonen er hele kodomenet: <span class="math inline">\(\text{im} f = B\)</span>.</p>
<h2 id="lineærtransformasjoner-1">Lineærtransformasjoner</h2>
<p><strong>Definisjon.</strong> La <span class="math inline">\(V\)</span> og <span class="math inline">\(W\)</span> være vektorrom. En funksjon <span class="math inline">\(T: V \rightarrow W\)</span> er en <em>lineærtransformasjon</em> hvis den oppfyller følgende to kriterier:</p>
<ol>
<li><p><span class="math inline">\(T(u + v) = T(u) + T(v)\)</span> for alle <span class="math inline">\(u\)</span> og <span class="math inline">\(v\)</span> i <span class="math inline">\(V\)</span>.</p></li>
<li><p><span class="math inline">\(T(cu) = c \cdot T(u)\)</span> for alle vektorer <span class="math inline">\(u\)</span> i <span class="math inline">\(V\)</span> og alle skalarer <span class="math inline">\(c\)</span>.</p></li>
</ol>
<p><strong>Teorem 8.3.</strong> Hvis <span class="math inline">\(T: V \rightarrow W\)</span> er en lineærtrasformasjon, så oppfyller den følgende:</p>
<ol>
<li><p>En lineærkombinasjon i <span class="math inline">\(V\)</span> sendes til den tilsvarende lineærkombinasjonen i <span class="math inline">\(W\)</span>: <span class="math display">\[T(c_1v_1 + c_2v_2 + \cdots + c_rv_r) = c_1 \cdot T(v_1) + c_2 \ T(v_2) + \cdots + c_2 \cdots T(v_r)\]</span></p></li>
<li><p>Nullvektoren i <span class="math inline">\(V\)</span> sendes til nullvektoren i <span class="math inline">\(W\)</span>: <span class="math display">\[T(0) = 0\]</span></p></li>
</ol>
<p><strong>Teorem 8.4.</strong> La <span class="math inline">\(V\)</span> og <span class="math inline">\(W\)</span> være endeligdimensjonale vektorrom og la <span class="math inline">\(T: V \rightarrow W\)</span> være en lineærtransformasjon. Anta <span class="math inline">\(B = (b_1, \dots, b_n)\)</span> er en basis for <span class="math inline">\(V\)</span>. Da er <span class="math inline">\(T\)</span> fullstendig bestemt av bildene <span class="math inline">\(T(b1), \dots, T(b_n)\)</span> av basisvektorene.</p>
<h2 id="kjerne-og-bilde">Kjerne og bilde</h2>
<p><strong>Definisjon.</strong> La <span class="math inline">\(T: V \rightarrow W\)</span> være en lineærtransformasjon. <em>Kjernen</em> til <span class="math inline">\(T\)</span> er mengden av alle vektorer i <span class="math inline">\(V\)</span> som blir sendt til nullvektoren i <span class="math inline">\(W\)</span>: <span class="math display">\[\text{ker}\,T = \{\,v \in V \ | \ T(v) = 0 \}\]</span> <strong>Teorem 8.8.</strong> La <span class="math inline">\(T: V \rightarrow W\)</span> være en lineærtransformasjon.</p>
<ol>
<li><p>Kjernen <span class="math inline">\(\text{ket}\,T\)</span> er et underrom av <span class="math inline">\(V\)</span>.</p></li>
<li><p>Bildet <span class="math inline">\(\text{im}\,T\)</span> er et underrom av <span class="math inline">\(W\)</span>.</p></li>
</ol>
<p><strong>Teorem 8.9.</strong> En lineærtransformasjon <span class="math inline">\(T: V \rightarrow W\)</span> er injektiv hvis og bare hvis <span class="math inline">\(\text{ker}\,T=\{\,0\,\}\)</span>.</p>
<h2 id="lineærtransformasjoner-gitt-ved-matriser">Lineærtransformasjoner gitt ved matriser</h2>
<p><strong>Teorem 8.11.</strong> La <span class="math inline">\(A\)</span> være en <span class="math inline">\(m \times n\)</span>-matrise, og la <span class="math inline">\(T: \mathbb{R}^n \rightarrow \mathbb{R}^m\)</span> være en lineærtransformasjon gitt ved <span class="math inline">\(T(x) = Ax\)</span>. Da er <span class="math display">\[\text{ker}\,T = \text{Null}\,A \quad \text{og} \quad \text{im}\,T=\text{Col}\,A\]</span> <strong>Teorem 8.13.</strong> La <span class="math inline">\(T: \mathbb{R}^n \rightarrow \mathbb{R}^m\)</span> være en lineærtransformasjon. Da er <span class="math display">\[T(x) = Ax \quad \text{for alle } x \text{ i } \mathbb{R}^n\]</span> der <span class="math inline">\(A\)</span> er <span class="math inline">\(m \times n\)</span>-matrisen gitt ved <span class="math display">\[[ \ T(e_1) \ T(e_2) \ \cdots \ T(e_n) \ ]\]</span> hvor <span class="math inline">\((e_1, e_2, \dots, e_n)\)</span> er standardmatrisen for <span class="math inline">\(\mathbb{R}^n\)</span>.<br />
<br />
<strong>Definisjon.</strong> Matrisen <span class="math inline">\(A\)</span> i teorem 8.13 kalles <em>standardmatrisen</em> til lineærtransformasjonen <span class="math inline">\(T\)</span>.<br />
<br />
<strong>Teorem 8.14.</strong> La <span class="math inline">\(T: \mathbb{R}^n \rightarrow \mathbb{R}^m\)</span> være en lineærtransformasjon og la <span class="math inline">\(A\)</span> være standardmatrisen til <span class="math inline">\(T\)</span>. Da vet vi:</p>
<ol>
<li><p><span class="math inline">\(T\)</span> er surjektiv hvis og bare hvis kolonnene i <span class="math inline">\(A\)</span> utspenner hele <span class="math inline">\(\mathbb{R}^m\)</span>.</p></li>
<li><p><span class="math inline">\(T\)</span> er injektiv hvis og bare hvis kolonnene i <span class="math inline">\(A\)</span> er lineært uavhengige.</p></li>
</ol>
<h2 id="lineærtransformasjoner-og-basiser">Lineærtransformasjoner og basiser</h2>
<p><strong>Teorem 8.15.</strong> La <span class="math inline">\(V\)</span> og <span class="math inline">\(W\)</span> være endeligdimensjonale vektorrom, og la <span class="math inline">\(B\)</span> og <span class="math inline">\(C\)</span> være basiser for henholdsvis <span class="math inline">\(V\)</span> og <span class="math inline">\(W\)</span>. La <span class="math inline">\(T: V \rightarrow W\)</span> være en lineærtransformasjon. Da finnes en matrise <span class="math inline">\(A\)</span> slik at <span class="math display">\[[ \ T(x) \ ]_C = A \cdot [ \ x \ ]_B\]</span> for alle vektorer <span class="math inline">\(x\)</span> i <span class="math inline">\(V\)</span>.<br />
<br />
<strong>Teorem 8.16.</strong> La <span class="math inline">\(V\)</span> være et <span class="math inline">\(n\)</span>-dimensjonalt vektorrom, og la <span class="math inline">\(B = (b_1, b_2, \dots, b_n)\)</span> og <span class="math inline">\(C = (c_1, c_2, \dots, c_n)\)</span> være to basiser for <span class="math inline">\(V\)</span>. Da finnes en <span class="math inline">\(n \times n\)</span>-matrise <span class="math inline">\(A\)</span> slik at <span class="math display">\[[ \ x \ ]_C = A \cdot [ \ x \ ]_B\]</span> for alle vektorer <span class="math inline">\(x\)</span> i <span class="math inline">\(V\)</span>.</p>
<h2 id="isomorfi">Isomorfi</h2>
<p><strong>Definisjon.</strong> La <span class="math inline">\(T: V \rightarrow W\)</span> være en lineærtransformasjon. En <em>invers</em> til <span class="math inline">\(T\)</span> er en lineærtransformasjon <span class="math inline">\(S: W \rightarrow V\)</span> som er slik at <span class="math display">\[\begin{aligned}
    S(T(v)) = v \quad &amp;\text{for alle } v \text{ i } V \text{, og} \\
    T(S(w)) = w \quad &amp;\text{for alle } w \text{ i } W\end{aligned}\]</span> <strong>Definisjon.</strong> Hvis <span class="math inline">\(T: V \rightarrow W\)</span> er en lineærtransformasjon som har en invers, så er <span class="math inline">\(T\)</span> en <em>isomorfi</em>. Da sier vi dessuten at vektorrommene <span class="math inline">\(V\)</span> og <span class="math inline">\(W\)</span> er <em>isomorfe</em>, og vi skriver <span class="math inline">\(V \cong W\)</span>.<br />
<br />
<strong>Teorem 8.19.</strong> En lineærtransformasjon er en isomorfi hvis og bare hvis den er både injektiv og surjektiv.<br />
<br />
<strong>Teorem 8.22.</strong> Hvis <span class="math inline">\(V\)</span> er et <span class="math inline">\(n\)</span>-dimensjonalt komplekst vektorrom, så er <span class="math inline">\(V\)</span> isomorft med <span class="math inline">\(\mathbb{C}^n\)</span>. Og hvis <span class="math inline">\(W\)</span> er et <span class="math inline">\(n\)</span>-dimensjonalt reelt vektorrom, så er <span class="math inline">\(W\)</span> isomorft med <span class="math inline">\(\mathbb{R}^n\)</span>.</p>
<h1 id="projeksjon">Projeksjon</h1>
<p>Finner du ikke det du ser etter? Se <a href="https://www.math.ntnu.no/emner/TMA4110/2020h/notater/9-projeksjon.pdf">Wiki</a>.</p>
<h2 id="skalarproduktet-i-rn">Skalarproduktet i <span class="math inline">\(R^n\)</span></h2>
<p><strong>Teorem 9.3.</strong> Den ortogonale projeksjonen på en vektor <span class="math inline">\(v\)</span>, <span class="math display">\[\begin{gathered}
    P_v : \mathbb{R}^n \rightarrow \mathbb{R}^n \\
    P_v(w)=\frac{v \cdot w}{v \cdot v}\,v\end{gathered}\]</span> er en lineærtransformasjon.</p>
<h2 id="indreproduktrom">Indreproduktrom</h2>
<p><strong>Definisjon.</strong> La <span class="math inline">\(V\)</span> være et reelt vektorrom. Et <em>indreproduktrom</em> i <span class="math inline">\(V\)</span> er en operasjon som tar inn to vektorer, <span class="math inline">\(v\)</span> og <span class="math inline">\(w\)</span>, for å gi ut et reelt tall <span class="math inline">\(\langle v, w \rangle\)</span>. Operasjonen tilfredsstiller <span class="math display">\[\begin{aligned}
    &amp;\langle v, w \rangle = \langle w, v \rangle &amp;\text{(symmetri)} \\
    &amp;\langle v, (aw + bu) \rangle = a \langle v, w \rangle + b \langle v, u \rangle &amp;\text{(linearitet)} \\
    &amp;\langle v, v \rangle \geq 0 \text{, og } \langle v, v \rangle = 0 \text{ kun hvis } v = 0 &amp;\text{(positivitet)}\end{aligned}\]</span> Vi sier at <span class="math inline">\(V\)</span>, sammen med et valgt indreprodukt, er et <em>indreproduktrom</em>.<br />
<br />
<strong>Teorem 9.6.</strong> (Pytagoras). La <span class="math inline">\(V\)</span> være et reelt indreproduktrom. Dersom vektorene <span class="math inline">\(v\)</span> og <span class="math inline">\(w\)</span> er ortogonale, er <span class="math display">\[\lVert v + w \rVert = \lVert v \rVert^2 + \lVert w \rVert^2\]</span> <strong>Teorem 9.7.</strong> (Cauchy-Schwarz). La <span class="math inline">\(V\)</span> være et reelt indreproduktrom. Alle vektorer <span class="math inline">\(v\)</span> og <span class="math inline">\(w\)</span> tilfredstiller <span class="math display">\[| \langle v, w \rangle | \leq \lVert v \rVert \lVert w \rVert\]</span> <strong>Teorem 9.8.</strong> La <span class="math inline">\(V\)</span> være et indreproduktrom. Den ortogonale projeksjonen på en vektor <span class="math inline">\(v\)</span>, <span class="math display">\[\begin{gathered}
    P_v : \mathbb{R}^n \rightarrow \mathbb{R}^n \\
    P_v(w)=\frac{\langle v, w \rangle}{\langle v, v \rangle}\,v\end{gathered}\]</span> er en lineærtransformasjon.<br />
<br />
<strong>Teorem 9.9.</strong> La <span class="math inline">\(v\)</span> og <span class="math inline">\(w\)</span> være to vektorer i et indreproduktrom <span class="math inline">\(V\)</span>. Da er <span class="math inline">\(P_v(w)\)</span> og <span class="math inline">\(w-P_v(w)\)</span> ortogonale.</p>
<h2 id="ortogonal-projeksjon">Ortogonal projeksjon</h2>
<p><strong>Definisjon.</strong> En <em>ortogonal mengde</em> er en mengde av ikke-null vektorer <span class="math inline">\(u_1, u_2, \dots, u_n\)</span>, slik at <span class="math display">\[\langle u_i, u_k \rangle = 0\]</span> for alle vektorer <span class="math inline">\(u_i\)</span> og <span class="math inline">\(u_k\)</span> i mengden med <span class="math inline">\(i \neq k\)</span>. Dersom i tillegg <span class="math inline">\(\lVert u_j \rVert = 1\)</span> for alle vektorene, sier vi at mengden er <em>ortonormal</em>.<br />
<br />
<strong>Teorem 9.10.</strong> En ortogonal mengde er lineært uavhengig.<br />
<br />
<strong>Definisjon.</strong> Dersom en ortogonal mengde <span class="math inline">\(u_1, u_2, \dots, u_n\)</span> i <span class="math inline">\(V\)</span> også er en basis, sier vi at mengden er en <em>ortogonal basis</em> for <span class="math inline">\(V\)</span>.<br />
<br />
<strong>Teorem 9.12.</strong> Koordinatene til <span class="math inline">\(v\)</span> i en ortogonal basis <span class="math inline">\(u_1, u_2, \dots, u_n\)</span> er <span class="math display">\[\begin{aligned}
    v &amp;= P_{u_1}(v) + P_{u_2}(v) + \dots + P_{u_n}(v) \\
    &amp;= \frac{\langle u_1, v \rangle}{\langle u_1, u_1 \rangle}\,u_1
    + \frac{\langle u_2, v \rangle}{\langle u_2, u_2 \rangle}\,u_2
    + \dots + \frac{\langle u_n, v \rangle}{\langle u_n, u_n \rangle}\,u_n\end{aligned}\]</span> <strong>Teorem 9.13.</strong> La <span class="math inline">\(u_1, u_2, \dots, u_n\)</span> være en ortogonal basis for <span class="math inline">\(U\)</span>, et underrom av <span class="math inline">\(V\)</span>. Punktet <span class="math display">\[\begin{aligned}
    P_U(v) &amp;= P_{u_1}(v) + P_{u_2}(v) + \dots + P_{u_n}(v) \\
    &amp;= \frac{\langle u_1, v \rangle}{\langle u_1, u_1 \rangle}\,u_1
    + \frac{\langle u_2, v \rangle}{\langle u_2, u_2 \rangle}\,u_2
    + \dots + \frac{\langle u_n, v \rangle}{\langle u_n, u_n \rangle}\,u_n\end{aligned}\]</span> er et punktet i <span class="math inline">\(U\)</span> som har kortest avstand til <span class="math inline">\(v\)</span>, altså <span class="math display">\[\lVert v - P_U(v) \rVert = \text{min}_{u \in U} \lVert v - u \rVert\]</span> <strong>Teorem 9.15.</strong> La <span class="math inline">\(u_1, u_2, \dots, u_n\)</span> være en basis for et underrom <span class="math inline">\(U\)</span>. En vektor <span class="math inline">\(v\)</span> er i det ortogonale komplementet til <span class="math inline">\(U\)</span> hvis og bare hvis <span class="math inline">\(\langle u_i, v \rangle = 0\)</span> for alle <span class="math inline">\(i\)</span>.<br />
<br />
<strong>Teorem 9.17.</strong> La <span class="math inline">\(U\)</span> være et underrom av et indreproduktrom <span class="math inline">\(V\)</span>. Da gjelder <span class="math display">\[\dim{U} + \dim{U^\perp} = \dim{V}\]</span> <strong>Teorem 9.18.</strong> La <span class="math inline">\(A\)</span> være en <span class="math inline">\(m \times n\)</span>-matrise. Da vet vi <span class="math display">\[\begin{aligned}
    (\text{Col}\,A)^\perp &amp;= \text{Null}\,A^T \\
    (\text{Null}\,A)^\perp &amp;= \text{Col}\,A^T\end{aligned}\]</span></p>
<h2 id="finne-en-ortogonal-basis-gram-schmidts-metode">Finne en ortogonal basis: Gram-Schmidts metode</h2>
<p><strong>Teorem 9.19.</strong> Mengden <span class="math inline">\(u_1, u_2, \dots, u_n\)</span> gitt ved <span class="math display">\[\begin{aligned}
    u_k &amp;= v_k - \sum_{j=1}^{k-1}\,P_{u_j}(v_k) \\
    &amp;= v_k - \sum_{j=1}^{k-1}\,\frac{\langle u_j, v_k \rangle}{\langle u_j, u_j \rangle}\,u_j\end{aligned}\]</span> er en ortogonal basis for rommet utspent av <span class="math inline">\(v_1, v_2, \dots, v_n\)</span>.</p>
<h2 id="beregne-den-ortogonale-basisen">Beregne den ortogonale basisen</h2>
<p>La <span class="math inline">\(U\)</span> være et underrom til et indreproduktrom <span class="math inline">\(V\)</span>. Her er en metode for å regne ut den ortogonale projeksjonen <span class="math inline">\(P_U\)</span>:</p>
<ol>
<li><p>Ta utgangspunkt i en basis <span class="math inline">\(v_1, v_2, \dots, v_n\)</span> som spenner ut <span class="math inline">\(U\)</span>.</p></li>
<li><p>Bruk så Gram-Schmidt til å finne en ortogonal basis <span class="math inline">\(u_1, u_2, \dots, u_n\)</span> for <span class="math inline">\(U\)</span>.</p></li>
<li><p>Da kan man, for alle <span class="math inline">\(v\)</span> i <span class="math inline">\(V\)</span>, beregne den ortogonale projeksjonen <span class="math display">\[P_U(x) = P_{u_1}(x) + P_{u_2}(x) + \dots + P_{u_n}(x)\]</span></p></li>
<li><p>Hvis <span class="math inline">\(V = \mathbb{R}^n\)</span>, så får vi standardmatrisen <span class="math display">\[[\, P_U \,] = [\, P_U(e_1) \ P_U(e_2) \ \dots \ P_U(e_n) \, ]\]</span> og da kan man, for alle <span class="math inline">\(x\)</span> i <span class="math inline">\(V\)</span>, beregne den ortogonale projeksjonen <span class="math inline">\(P_U(x) = [\, P_U \,]x\)</span>.</p></li>
</ol>
<h2 id="indreproduktrom-mellom-funksjoner">Indreproduktrom mellom funksjoner</h2>
<p><strong>Teorem 9.22.</strong> Operasjonen <span class="math display">\[\langle f, g \rangle = \frac{1}{b - a} \int_a^b f(x)\,g(x)\,dx\]</span> er et indreprodukt på <span class="math inline">\(C([a, b])\)</span>.<br />
<br />
<strong>Teorem 9.27.</strong> Vektorene <span class="math inline">\(1\)</span>, <span class="math inline">\(\cos{nx}\)</span> og <span class="math inline">\(\sin{mx}\)</span>, hvor <span class="math inline">\(n, m = 1, 2, 3, \dots,\)</span> er parvis ortogonale i <span class="math inline">\(C_s([-\pi, \pi])\)</span>.</p>
<h2 id="komplekse-indreprodukt">Komplekse indreprodukt</h2>
<p><strong>Definisjon.</strong> La <span class="math inline">\(V\)</span> være et komplekst vektorrom. Et <em>indreproduktrom</em> i <span class="math inline">\(V\)</span> er en operasjon som tar inn to vektorer, <span class="math inline">\(v\)</span> og <span class="math inline">\(w\)</span>, for å gi ut et komplekst tall <span class="math inline">\(\langle v, w \rangle\)</span>. Operasjonen tilfredsstiller <span class="math display">\[\begin{aligned}
    &amp;\langle v, w \rangle = \overline{ \langle w, v \rangle } &amp;\text{(konj. sym.)} \\
    &amp;\langle v, (aw + bu) \rangle = a \langle v, w \rangle + b \langle v, u \rangle &amp;\text{(linearitet)} \\
    &amp;\langle v, v \rangle \geq 0 \text{, og } \langle v, v \rangle = 0 \text{ kun hvis } v = 0 &amp;\text{(positivitet)}\end{aligned}\]</span> Vi sier at <span class="math inline">\(V\)</span>, sammen med et valgt indreprodukt, er et <em>indreproduktrom</em>.<br />
<br />
<strong>Teorem 9.29.</strong> Alt som gjelder for reelle indreproduktrom, med unntak av vinkelen, gjelder også for komplekse indreproduktrom.<br />
<br />
<strong>Teorem 9.33.</strong> La <span class="math inline">\(A\)</span> være en kompleks <span class="math inline">\(m \times n\)</span>-matrise. Da har vi <span class="math display">\[\begin{aligned}
    (\text{Col}\,A)^\perp &amp;= \text{Null}\,A^* \\
    (\text{Null}\,A)^\perp &amp;= \text{Col}\,A^*\end{aligned}\]</span></p>
<h1 id="egenverdier-og-egenvektorer">Egenverdier og egenvektorer</h1>
<p>Finner du ikke det du ser etter? Se <a href="https://www.math.ntnu.no/emner/TMA4110/2020h/notater/10-egenverdier-og-egenvektorer.pdf">Wiki</a>.</p>
<h2 id="definisjon-av-egenverdier-og-egenvektorer">Definisjon av egenverdier og egenvektorer</h2>
<p><strong>Definisjon.</strong> La <span class="math inline">\(T: V \rightarrow V\)</span> være en lineærtransformasjon. En skalar <span class="math inline">\(\lambda\)</span> er en <em>egenverdi</em> for <span class="math inline">\(T\)</span> hvis det finnes en vektor <span class="math inline">\(v \neq 0\)</span> i <span class="math inline">\(V\)</span> slik at <span class="math display">\[T(v) = \lambda \cdot v\]</span> Vektoren <span class="math inline">\(v\)</span> kalles en <em>egenvektor</em> for <span class="math inline">\(T\)</span> som hører til egenverdien <span class="math inline">\(\lambda\)</span>. Når <span class="math inline">\(T\)</span> er gitt ved en <span class="math inline">\(n \times n\)</span>-matrise <span class="math inline">\(A\)</span>, så sier vi også at <span class="math inline">\(\lambda\)</span> er en egenverdi for <span class="math inline">\(A\)</span> og <span class="math inline">\(v\)</span> er egenvektor for <span class="math inline">\(A\)</span> som hører til egenverdien <span class="math inline">\(\lambda\)</span>.</p>
<h2 id="noen-generelle-observasjoner">Noen generelle observasjoner</h2>
<p><strong>Teorem 10.3.</strong> Anta at <span class="math inline">\(\lambda\)</span> er en egenverdi for en lineærtransformasjon <span class="math inline">\(T: V \rightarrow V\)</span>, og at <span class="math inline">\(v\)</span> er en tilhørende egenvektor. Da er alle multipletter <span class="math inline">\(cv\)</span> av vektoren <span class="math inline">\(v\)</span>, der <span class="math inline">\(c\)</span> er et tall som ikke er <span class="math inline">\(0\)</span>, også egenvektorer som hører til egenverdien <span class="math inline">\(\lambda\)</span>. Med andre ord er alle vektorer i mengden <span class="math inline">\(\text{Sp}\{v\}\)</span>, unntatt nullvektoren, egenvektorer som hører til egenverdien <span class="math inline">\(\lambda\)</span>.<br />
<br />
<strong>Teorem 10.4.</strong> En <span class="math inline">\(n \times n\)</span>-matrise <span class="math inline">\(A\)</span> har <span class="math inline">\(0\)</span> som egenverdi hvis og bare hvis den ikke er inverterbar.<br />
<br />
<strong>Teorem 10.5.</strong> La <span class="math inline">\(T: V \rightarrow V\)</span> være en lineærtransformasjon. La <span class="math inline">\(v_1, v_2, \dots, v_t\)</span> være egenvektorer til <span class="math inline">\(T\)</span> som hører til forskjellige egenverdier <span class="math inline">\(\lambda_1, \lambda_2, \dots, \lambda_t\)</span>. Da er vektorene <span class="math inline">\(v_1, v_2, \dots, v_t\)</span> lineært uavhengige.<br />
<br />
<strong>Teorem 10.6.</strong> La <span class="math inline">\(V\)</span> være et <span class="math inline">\(n\)</span>-dimensjonalt vektorrom og <span class="math inline">\(T: V \rightarrow V\)</span> være en lineærtransforamsjon. Hvis <span class="math inline">\(T\)</span> har <span class="math inline">\(n\)</span> forskjellige egenverdier, så finnes det en basis for <span class="math inline">\(V\)</span> som består egenvektorer av <span class="math inline">\(T\)</span>.<br />
<br />
<strong>Teorem fra LF.</strong> La <span class="math inline">\(A\)</span> være en inverterbar matrise med egenverdi <span class="math inline">\(a\)</span>. Da er <span class="math inline">\(\frac{1}{a}\)</span> en egenverdi for <span class="math inline">\(A^{-1}\)</span>.</p>
<h2 id="hvordan-finne-egenverdier-og-egenvektorer">Hvordan finne egenverdier og egenvektorer?</h2>
<p><strong>Teorem 10.9.</strong> La <span class="math inline">\(A\)</span> være en <span class="math inline">\(n \times n\)</span>-matrise.</p>
<ol>
<li><p>Egenverdiene til <span class="math inline">\(A\)</span> er alle løsninger <span class="math inline">\(\lambda\)</span> av likningen <span class="math display">\[\det{(A - \lambda I_n)} = 0\]</span></p></li>
<li><p>Hvis <span class="math inline">\(\lambda\)</span> er en egenverdi for <span class="math inline">\(A\)</span>, så er de tilhørende egenvektrene gitt ved alle ikke-trivielle løsninger av likningen <span class="math display">\[(A - \lambda I_n ) \cdot x = 0\]</span></p></li>
</ol>
<p><strong>Definisjon.</strong> En <em>diagonalmatrise</em> er en kvadratisk matrise der alle tall utenfor diagonalen er <span class="math inline">\(0\)</span>.<br />
<br />
<strong>10.10.</strong> Egenverdiene til en diagonalmatrise er tallene på diagonalen.</p>
<h2 id="egenrom">Egenrom</h2>
<p><strong>Definisjon.</strong> La <span class="math inline">\(T: V \rightarrow V\)</span> være en lineærtransformasjon, og anta at <span class="math inline">\(\lambda\)</span> er en egenverdi for <span class="math inline">\(T\)</span>. Da er <em>egenrommet</em> til <span class="math inline">\(\lambda\)</span> mengden av alle egenvektorer som hører til <span class="math inline">\(\lambda\)</span>, samt nullvektoren; altså mengden <span class="math display">\[\{\, v \in V \ | \ T(v) = \lambda v \, \}\]</span> Dimensjonen til egenrommet kalles den <em>geometriske multiplisiteten</em> til <span class="math inline">\(\lambda\)</span>.</p>
<h2 id="eksistens-av-egenverdier">Eksistens av egenverdier</h2>
<p><strong>Teorem 10.14.</strong> En kompleks <span class="math inline">\(n \times n\)</span>-matrise <span class="math inline">\(A\)</span> har alltid <span class="math inline">\(n\)</span> egenverdier (talt med algebraisk multiplisitet).<br />
<br />
<strong>Teorem 10.15.</strong> For en lineærtransformasjon <span class="math inline">\(T: \mathbb{R}^n \rightarrow \mathbb{R}^n\)</span> er det mulig at den ikke har noen reelle egenverdier. Hvis <span class="math inline">\(n\)</span> er et oddetall, har <span class="math inline">\(T\)</span> minst én reell egenverdi.<br />
<br />
<strong>Teorem 10.16.</strong> De komplekse egenverdiene til en reell matrise kommer i komlekskonjugerte par.<br />
<br />
<strong>Teorem 10.20.</strong> Egenrommet har dimensjon større enn eller lik <span class="math inline">\(1\)</span> og mindre enn eller lik den algebraiske multiplisiteten til egenverdien.</p>
<h1 id="diagonalisering">Diagonalisering</h1>
<p>Finner du ikke det du ser etter? Se <a href="https://www.math.ntnu.no/emner/TMA4110/2020h/notater/11-diagonalisering.pdf">Wiki</a>.<br />
<br />
<strong>Definisjon.</strong> Vi sier at en <span class="math inline">\(n \times n\)</span>-matrise <span class="math inline">\(A\)</span> er <em>diagonaliserbar</em> hvis det finnes en diagonalmatrise <span class="math inline">\(D\)</span> og en inverterbar matrise <span class="math inline">\(P\)</span> slik at <span class="math display">\[A = PDP^{-1}\]</span> Vi sier da at <span class="math inline">\(P\)</span> diagonaliserer <span class="math inline">\(A\)</span>.<br />
<br />
<strong>Teorem 11.2.</strong> En <span class="math inline">\(n \times n\)</span>-matrise <span class="math inline">\(A\)</span> her diagonaliserbar hvis og bare hvis <span class="math inline">\(A\)</span> har <span class="math inline">\(n\)</span> lineært uavhengige egenvektorer.<br />
<br />
<strong>Teorem 11.3.</strong> Hvis en <span class="math inline">\(n \times n\)</span>-matrise <span class="math inline">\(A\)</span> har <span class="math inline">\(n\)</span> forskjellige egenverdier, så er <span class="math inline">\(A\)</span> diagonaliserbar.<br />
<br />
<strong>Teorem 11.4.</strong> En <span class="math inline">\(n \times n\)</span>-matrise <span class="math inline">\(A\)</span> er diagonaliserbar hvis og bare hvis <span class="math inline">\(A\)</span> har <span class="math inline">\(n\)</span> egenverdier og dimensjonen til egenrommet til hver egenverdi <span class="math inline">\(\lambda\)</span> er lik den algebraiske multiplisiteten til <span class="math inline">\(\lambda\)</span>.<br />
<br />
<strong>Teorem 11.5.</strong> En kompleks <span class="math inline">\(n \times n\)</span>-matrise <span class="math inline">\(A\)</span> er diagonaliserbar hvis og bare hvis det finnes en basis for <span class="math inline">\(\mathbb{C}^n\)</span> som kun består av egenvektorer for <span class="math inline">\(A\)</span>. En reell <span class="math inline">\(n \times n\)</span>-matrise <span class="math inline">\(A\)</span> er diagonaliserbar som en reell matrise hvis og bare hvis det finnes en basis for <span class="math inline">\(\mathbb{R}^n\)</span> som kun består av egenvektorer for <span class="math inline">\(A\)</span>.<br />
<br />
<strong>Teorem 11.6.</strong> La <span class="math inline">\(T: V \rightarrow V\)</span> være en lineærtransformasjon. Vi antar at <span class="math inline">\(T\)</span> er diagonaliserbar og at <span class="math inline">\(B = (v_1, \dots, v_n)\)</span> er en basis som består av <span class="math inline">\(T\)</span> sine egenvektorer. Da er matrisen som beskriver <span class="math inline">\(T\)</span> med hensyn på basisen <span class="math inline">\(B\)</span> en diagonalmatrise <span class="math inline">\(D\)</span>, med egenverdierne <span class="math inline">\(\lambda_1, \dots, \lambda_n\)</span> til <span class="math inline">\(T\)</span> på diagonalen.</p>
<h2 id="mer-om-komplekse-egenverdier">Mer om komplekse egenverdier</h2>
<p><strong>Teorem 11.11.</strong> La <span class="math inline">\(A\)</span> være en reell <span class="math inline">\(2 \times 2\)</span>-matrise med kompleks egenverdi <span class="math inline">\(\lambda = a - bi\)</span>, med <span class="math inline">\(b \neq 0\)</span>, og la <span class="math inline">\(v \in \mathbb{C}^2\)</span> være en egenvektor som hører til <span class="math inline">\(\lambda\)</span>. Da kan vi faktorisere <span class="math inline">\(A\)</span> på følgende måte: <span class="math display">\[A = PCP^{-1} \ \text{med} \ P = [\, \text{Re}\,v \ \text{Im}\,v\,]\]</span> og <span class="math display">\[C = \begin{bmatrix}
    a &amp; -b \\ b &amp; a
\end{bmatrix}\]</span></p>
<h2 id="symmetriske-matriser">Symmetriske matriser</h2>
<p><strong>Definisjon.</strong> En reell matrise kalles <em>symmetrisk</em> dersom <span class="math inline">\(A = A^T\)</span>.<br />
<br />
<strong>Teorem 11.14.</strong> La <span class="math inline">\(A\)</span> være en symmetrisk <span class="math inline">\(n \times n\)</span>-matrise. Da har <span class="math inline">\(A\)</span> <span class="math inline">\(n\)</span> reelle egenverdier (talt med multiplisitet) og <span class="math inline">\(A\)</span> er diagonaliserbar (som en reell matrise).</p>
<h2 id="symmetri-og-ortogonalitet">Symmetri og ortogonalitet</h2>
<p><strong>Teorem 11.16.</strong> La <span class="math inline">\(A\)</span> være en symmetrisk <span class="math inline">\(n \times n\)</span>-matrise. Egenvektorene til <span class="math inline">\(A\)</span> tilhørende to distinkte egenverdier er ortogonale.<br />
<br />
<strong>Definisjon.</strong> En <span class="math inline">\(n \times n\)</span>-matrise er <em>ortogonalt diagonaliserbar</em> dersom den har <span class="math inline">\(n\)</span> ortogonale egenvektorer.<br />
<br />
<strong>Teorem 11.17.</strong> En reell <span class="math inline">\(n \times n\)</span>-matrise er ortogonalt diagonaliserbar hvis og bare hvis den er symmetrisk.<br />
<br />
<strong>Definisjon.</strong> En <span class="math inline">\(n \times n\)</span>-matrise <span class="math inline">\(A\)</span> kalles <em>hermitsk</em> hvis <span class="math display">\[A = A^*\]</span> <strong>Teorem 11.18.</strong> En hermitsk <span class="math inline">\(n \times n\)</span>-matrise har <span class="math inline">\(n\)</span> reelle egenverdier (talt med multiplisitet) og er ortogonalt diagonaliserbar.</p>
<h1 id="interpolasjon-regresjon-og-markovkjeder">Interpolasjon, regresjon og markovkjeder</h1>
<p>Finner du ikke det du ser etter? Se <a href="https://www.math.ntnu.no/emner/TMA4110/2020h/notater/12%20-%20Interpolasjon,%20regresjon%20og%20markovkjeder.pdf">Wiki</a>.</p>
<h2 id="minste-kvadraters-metode">Minste kvadraters metode</h2>
<p><strong>Definisjon.</strong> Vi kaller <span class="math inline">\(\hat{x}\)</span> den <em>minste kvadraters løsning</em> for <span class="math inline">\(Ax = b\)</span>.<br />
<br />
<strong>Teorem 12.1.</strong> La <span class="math inline">\(A\)</span> være en <span class="math inline">\(m \times n\)</span>-matrise og <span class="math inline">\(b\)</span> være en kolonne vektor i <span class="math inline">\(\mathbb{R}^m\)</span>. Mengden av minste kkvadraters løsninger for systemet <span class="math inline">\(Ax = b\)</span> er lik løsningsmengden for systemet <span class="math display">\[A^T(Ax-b)=0\]</span> Hvis <span class="math inline">\(n \times n\)</span>-matrisen <span class="math inline">\(A^TA\)</span> er inverterbar, finnes det for hver <span class="math inline">\(b\)</span>, en unik minste kvadraters løsning <span class="math inline">\(\hat{x}\)</span> for systemet <span class="math inline">\(Ax = b\)</span>.</p>
<h2 id="markovkjeder">Markovkjeder</h2>
<p><strong>Definisjon.</strong> En <em>sannsynlighetsvektor</em> er en vektor <span class="math inline">\(v\)</span> i <span class="math inline">\(\mathbb{R}^n\)</span> der alle koordinatene er større eller lik <span class="math inline">\(0\)</span> og summen av koordinatene er lik <span class="math inline">\(1\)</span>. En <span class="math inline">\(n \times n\)</span>-matrise <span class="math inline">\(M\)</span> kalles en <em>stokastisk matrise</em> hvis kolonnene i <span class="math inline">\(M\)</span> er sannsynlighetsvektorer.<br />
<br />
<strong>Definisjon.</strong> La <span class="math inline">\(M\)</span> være en stokastisk matrise og <span class="math inline">\(x_0\)</span> en sannsynlighetsvektor. Vi kalles følgen av vektorene <span class="math display">\[\{\, x_n \,\} \text{ for } n = 0, 1, 2, \dots\]</span> en <em>Markovkjede.</em><br />
<br />
<strong>Teorem 12.8.</strong> En stokastisk matrise <span class="math inline">\(M\)</span> har alltid <span class="math inline">\(\lambda = 1\)</span> som egenverdi.<br />
<br />
<strong>Definisjon.</strong> La <span class="math inline">\(M\)</span> være en stokastisk matrise. En egenvektor for <span class="math inline">\(M\)</span> som hører til egenverdi <span class="math inline">\(1\)</span> og er en sannsynlighetsvektor kalles en <em>likevektsvektor</em>.<br />
<br />
<strong>Definisjon.</strong> En stokastisk matrise <span class="math inline">\(M\)</span> kalles <em>regulær</em> hvis det finnes en <span class="math inline">\(k \geq 1\)</span> slik at alle elementene i <span class="math inline">\(M^k\)</span> er større enn 0.<br />
<br />
<strong>Teorem 12.10.</strong> La <span class="math inline">\(M\)</span> være en regulær stokastisk matrise. Da har <span class="math inline">\(M\)</span> en unik likevekstvektor <span class="math inline">\(q\)</span>. For enhver utgangssannsynlighetsvektor <span class="math inline">\(x_0\)</span> konvergerer Markovkjeden <span class="math inline">\(\{\, x_n \,\}\)</span> til <span class="math inline">\(q\)</span> når <span class="math inline">\(n \rightarrow \infty\)</span>.</p>
<h1 id="systemer-av-differensiallikninger">Systemer av differensiallikninger</h1>
<p>Finner du ikke det du ser etter? Se <a href="https://www.math.ntnu.no/emner/TMA4110/2020h/notater/13-system.pdf">Wiki</a>.</p>
<h2 id="vektorfunksjoner">Vektorfunksjoner</h2>
<p><strong>Teorem 13.5.</strong> Den deriverte av en vektorfunksjon <span class="math inline">\(y: \mathbb{R} \rightarrow \mathbb{R}^n\)</span> er lik vektorfunksjonen gitt ved å derivere hver komponent <span class="math display">\[y&#39;(t) = \begin{bmatrix}
    y_1&#39;(t) \\ y_2&#39;(t) \\ \vdots \\ y_n&#39;(t)
\end{bmatrix}\]</span></p>
<h2 id="systemer-av-differensiallikninger-1">Systemer av differensiallikninger</h2>
<p><strong>Teorem 13.7.</strong> (Superposisjonsprinsippet). Løsningene til <span class="math display">\[y&#39; = Ay\]</span> utgjør et vektorrom. Det vil si at dersom <span class="math inline">\(y_1\)</span> og <span class="math inline">\(y_2\)</span> er løsninger av systemet, så er <span class="math inline">\(c_1y_1 + c_2y_2\)</span> en løsning for alle reelle tall <span class="math inline">\(c_1\)</span> og <span class="math inline">\(c_2\)</span>.</p>
<h2 id="løsningsteknikk-for-reell-diagonaliserbare-matriser">Løsningsteknikk for reell diagonaliserbare matriser</h2>
<p><strong>Teorem 13.8.</strong> La <span class="math inline">\(A\)</span> være en diagonaliserbar <span class="math inline">\(n \times n\)</span>-matrise. Hvis <span class="math inline">\(v_1, v_2, \dots, v_n\)</span> er <span class="math inline">\(n\)</span> lineært uavhengige egenvektorer med tilhørende egenverdier <span class="math inline">\(\lambda_1, \lambda_2, \dots, \lambda_n\)</span>, så er <span class="math display">\[v_1e^{\lambda_1t}, v_2e^{\lambda_2t}, \dots, v_ne^{\lambda_nt}\]</span> en basis for løsningesrommet til <span class="math inline">\(y&#39; = Ay\)</span>. Med andre ord er <span class="math display">\[c_1v_1e^{\lambda_1t}, c_2v_2e^{\lambda_2t}, \dots, c_nv_ne^{\lambda_nt}\]</span> en generell løsning av <span class="math inline">\(y&#39; = Ay\)</span>.</p>
<h2 id="initialverdiproblemer">Initialverdiproblemer</h2>
<p><strong>Definisjon.</strong> Et <em>initialverdiproblem</em> er et system sammen med en intialbetingelse <span class="math inline">\(y(t_0) = y_0\)</span> hvor <span class="math inline">\(t_0\)</span> er et reelt tall og <span class="math inline">\(y_0\)</span> er en vektor i <span class="math inline">\(\mathbb{R}^n\)</span>.</p>
<h2 id="todimensjonale-system">Todimensjonale system</h2>
<p>Vi begrenser oss nå til todimensjonale system. Det betyr at <span class="math inline">\(A\)</span> har en reell <span class="math inline">\(2 \times 2\)</span>-matrise. Det er tre forskjellige tilfeller vi må skille mellom avhengig av hva slags egenverdier <span class="math inline">\(A\)</span> har. Husk at egenverdiene er røttene til det karakteristiske polynomet <span class="math inline">\(\det{(A-\lambda I)}\)</span>.</p>
<h3 id="tilfelle-1-to-forskjellige-reelle-røtter">Tilfelle 1: To forskjellige reelle røtter</h3>
<h3 id="tilfelle-2-to-komplekse-ikke-reelle-røtter">Tilfelle 2: To komplekse (ikke reelle) røtter</h3>
<p><strong>Teorem 13.16.</strong> Anta at <span class="math inline">\(\alpha + i \beta\)</span>, <span class="math inline">\(\beta \neq 0\)</span>, er en kompleks egenverdi til <span class="math inline">\(A\)</span> og la <span class="math inline">\(v\)</span> være en tilhørende kompleks egenvektor. Da danner <span class="math display">\[y_1(t) = e^{\alpha t}(\text{Re}(v)\cos{(\beta t)} - \text{Im}(v)\sin{(\beta t)})\]</span> og <span class="math display">\[y_2(t) = e^{\alpha t}(\text{Re}(v)\sin{(\beta t)} + \text{Im}(v)\cos{(\beta t)})\]</span> en basis for det reelle løsningsrommet til <span class="math inline">\(y&#39; = Ay\)</span>.</p>
<h3 id="tilfelle-3-én-reell-rot">Tilfelle 3: Én reell rot</h3>
<p><strong>Teorem 13.32.</strong> La <span class="math inline">\(A\)</span> være en reell <span class="math inline">\(2 \times 2\)</span>-matrise med reell egenverdi <span class="math inline">\(\lambda\)</span> med algebraisk multiplisitet 2. La <span class="math inline">\(v\)</span> være en egenvektor til <span class="math inline">\(\lambda\)</span> og la <span class="math inline">\(w\)</span> være en vektor som oppfyller <span class="math inline">\((A - \lambda I)w = v\)</span>. Da er løsningene til systemet <span class="math inline">\(y&#39; = Ay\)</span> på formen <span class="math display">\[y(t) = c_1e^{\lambda t}v + c_2e^{\lambda t}(tv + w)\]</span></p>
<h2 id="inhomogene-system">Inhomogene system</h2>
<p><strong>Teorem 13.24.</strong> Hvis <span class="math inline">\(y_p(t)\)</span> er en partikulær løsning av <span class="math display">\[y&#39;(t) = Ay(t) + f(t)\]</span> så er en generell løsning gitt av <span class="math display">\[y(t) = y_h(t) + y_p(t)\]</span> hvor <span class="math inline">\(y_h(t)\)</span> er en generell løsning av den tilhørende homogene likningen.<br />
<br />
<strong>Merk.</strong> Det forventes ikke at du kan finne partikulære løsninger av vilkårlige system. Du vil alltid få dem servert.</p>
<h1 id="andre-ordens-lineære-differensiallikninger">Andre ordens lineære differensiallikninger</h1>
<p>Finner du ikke det du ser etter? Se <a href="https://www.math.ntnu.no/emner/TMA4110/2020h/notater/14-andre-ordens-lineare-differensialligninger.pdf">Wiki</a>.</p>
<h2 id="noen-forberedelser">Noen forberedelser</h2>
<p>Vi skal behandle <em>andre ordens differensiallikninger med konstante koeffisienter</em>: <span class="math display">\[a_2y&#39;&#39;(t) + a_1y&#39;(t) + a_0y(t) = f(t)\]</span></p>
<h2 id="løsningsmengden-for-homogene-likninger-er-et-vektorrom">Løsningsmengden for homogene likninger er et vektorrom</h2>
<p><strong>Teorem 14.1.</strong> (Superposisjonsprinsippet). Løsningene til <span class="math display">\[a_2y&#39;&#39;(t) + a_1y&#39;(t) + a_0y(t) = 0\]</span> utgjør et reelt vektorrom. Det vil si at dersom <span class="math inline">\(y_1(t)\)</span> og <span class="math inline">\(y_2(t)\)</span> er løsninger, så er <span class="math inline">\(c_1y_1(t) + c_2y_2(t)\)</span> en løsning for alle reelle tall <span class="math inline">\(c_1\)</span> og <span class="math inline">\(c_2\)</span>.</p>
<h2 id="løsningsteknikk-for-homogene-likninger">Løsningsteknikk for homogene likninger</h2>
<p><strong>Teorem 14.2.</strong> Løsningsmengden til <span class="math display">\[a_2y&#39;&#39;(t) + a_1y&#39;(t) + a_0y(t) = 0\]</span> er et to-dimensjonalt reelt vektorrom som utspennes av følgende lineært uavhengige funksjoner. Dersom den karakteristiske likningen <span class="math inline">\(\lambda ^2 + a_1\lambda + a_0 = 0\)</span> har:</p>
<ul>
<li><p>to reelle løsninger <span class="math inline">\(\lambda_1 \neq \lambda_2\)</span>: <span class="math display">\[y_1(t) = e^{\lambda_1 t}, \ y_2(t) = e^{\lambda_2 t}\]</span></p></li>
<li><p>en kompleks løsning <span class="math inline">\(\lambda = a + bi\)</span> med <span class="math inline">\(b \neq 0\)</span>: <span class="math display">\[y_1(t) = e^{at}\cos{(bt)}, \ y_2(t) = e^{at}\sin{(bt)}\]</span></p></li>
<li><p>kun én reell løsning <span class="math inline">\(\lambda\)</span>: <span class="math display">\[y_1(t) = te^{\lambda t}, \ y_2(t) = e^{\lambda t}\]</span></p></li>
</ul>
<h2 id="løsningsteknikk-for-inhomogene-likninger">Løsningsteknikk for inhomogene likninger</h2>
<p><strong>Teorem 14.6.</strong> Alle løsninger til den inhomogene likningen er på formen <span class="math display">\[y(t) = y_p(t) + y_h(t)\]</span> der <span class="math inline">\(y_p(t)\)</span> er en partikulær løsning og <span class="math inline">\(y_h(t)\)</span> er en løsning til den tilsvarende homogene likningen.</p>
<h1 id="hjelpemidler">Hjelpemidler</h1>
<h2 id="wolfram-alpha">Wolfram Alpha</h2>
<p><a href="https://www.wolframalpha.com/input/?i=matrices">Wolfram Alpha - Matriser</a></p>
<h3 id="likningssystem">Likningssystem</h3>
<p>Et likningssystem kan skrives om til en matrise og løses ved å bruke den inverse.<br />
<br />
Eksempel: <span class="math display">\[\begin{gathered}
    3x - 2y + z = 7 \\
    x + y + 2z = 4 \\
    x - y - z = 0\end{gathered}\]</span> Kan gjøres om til matriselikningen <span class="math display">\[Ax = B\]</span> der <span class="math display">\[A = \begin{bmatrix}
    3 &amp; -2 &amp; 1 \\
    1 &amp; 1 &amp; 2 \\
    1 &amp; -1 &amp; -1
\end{bmatrix}, \quad
B = \begin{bmatrix}
    7 \\ 4 \\ 0
\end{bmatrix}\]</span> Hvis <span class="math inline">\(A\)</span> er inverterbar, så kan dette løses ved <span class="math display">\[\begin{aligned}
    Ax &amp;= B \\
    A^{-1}Ax &amp;= A^{-1}B \\
    x &amp; = A^{-1}B\end{aligned}\]</span> Dette kan løses direkte i Wolfram.</p>
<h3 id="gausseliminasjon">Gausseliminasjon</h3>
<p>’row reduce’ i Wolfram.<br />
<br />
<a href="https://www.wolframalpha.com/input/?i=row+reduce+%7B%7B1%2C+1%2C+4%2C+0%7D%2C+%7B1%2C+2%2C+6%2C+1%7D%2C+%7B0%2C+3%2C+6%2C+3%7D%7D">Eksempel</a></p>
</body>
</html>
